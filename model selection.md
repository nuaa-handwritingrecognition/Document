# 算法模型选用复盘

## 特征选取

特征选取：以用户第一个被捕捉到的触碰点为坐标原点，特征选取为用户的触碰点的横纵坐标(xi, yi)，以及当前触碰点的压力(pi)，一共3个特征。每隔20ms记录一次，一共记录3s，共150次。如果用户3s没写完则舍弃3s之后的数据，若用户在3s之内写完，则之后的数据全部使用最后一次捕捉到的用户特征填充。一共有3*150个特征，分别为x1,y1,p1,x2,p2,y2, ... x150,y150,p150。

## 数据预处理

 舍去逻辑：

- 剔除 [0,1] 区间之外的压力值
- 剔除 150 行全为 1 个数值

## 算法选用（LSTM）

LSTM：LSTM是RNN（循环神经网络）的一种变体，RNN的外部结构如下：

![img](https://upload-images.jianshu.io/upload_images/42741-d6749df8fb93b0b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在上面的示例图中，神经网络的模块，A，正在读取某个输入 x_i，并输出一个值 h_i。循环可以使得信息可以从当前步传递到下一步。

普通的RNN内部结构如下：

![img](https://upload-images.jianshu.io/upload_images/42741-9ac355076444b66f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

将输入和上一次的输出拼接，再加上非线性函数，与权值相乘后得到结果。

LSTM在普通RNN的基础上，添加了一个忘记门和选择记忆门，以便忘掉相隔时间太长的数据，LSTM内部结构如下

忘记门：它查看ht-1(前一个输出)和xt(当前输入)，并为单元格状态Ct-1(上一个状态)中的每个数字输出0和1之间的数字。1代表完全保留，而0代表彻底删除。

![img](https://upload-images.jianshu.io/upload_images/6592751-e64119ca92a040e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

选择记忆门：将新的信息选择性的记录到细胞状态中 

![img](https://upload-images.jianshu.io/upload_images/6592751-cd0200cc57a31fc2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

输出门：将隐藏层乘以权值，加上非线性函数，保存以便下一个细胞的使用。



![img](https://upload-images.jianshu.io/upload_images/6592751-331b3b7eb76d5b98.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

将三个门合并，结构如下：

![img](https://upload-images.jianshu.io/upload_images/42741-dd3d241fa44a71c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在本实验中，x0，x1……xt都是三维向量，三个维度分别是x，y坐标，压力值。t=149，W为权值矩阵，维度自定义，这里有一个隐藏层，神经元个数为100，所以维度为100*3

h是输出层，维度为2*1，是一个one-hot向量，ht为最终输出，也就是结果。二维向量中值最大的那个数的坐标就是最终的值。比如，输出为[0.8, 0.2]，最终值就为0。

## 选择原因

为什么选用LSTM？LSTM在自然语言处理、视频行为分析中有着广泛的应用，这些场景都有一个共同的特点：特征的时序性。也就是说，某一个时间点的数据和之前的数据有关联，从单个数据中不能分析出有效信息，必须结合之前的数据。例如，仅仅从某个篮球视频里的一张截图中，很难分辨一个运动员是正在接球还是在抛球，如果加入时序特征，分析这张照片之前的动作，可以很容易分辨出来。如果前几帧图画中球离人越来越远，则可以推断此人正在抛球，反之则在接球。

在本实验中，我们选取的特征也是具有时序特征的，在一个人写一个特定的字的过程中，某一个点的位置以及压力值与其前面几秒或者几毫秒的数据都有关系，所以选用LSTM是最自然，最直接的想法。

## 其他算法

### KNN

如下图，绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。

![img](https://s2.ax1x.com/2019/05/19/EjQxGq.jpg)

算法描述：

1）计算测试数据与各个训练数据之间的距离；

2）按照距离的递增关系进行排序；

3）选取距离最小的K个点；

4）确定前K个点所在类别的出现频率；

5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。

KNN结果：

| K    | FF   | FT   | TF   | TT   | 误登率   | 误杀率   |
| ---- | ---- | ---- | ---- | ---- | -------- | -------- |
| 5    | 1302 | 30   | 87   | 22   | 0.022523 | 0.798165 |
| 7    | 1307 | 25   | 89   | 20   | 0.018769 | 0.816514 |
| 9    | 1311 | 21   | 88   | 21   | 0.015766 | 0.807339 |
| 15   | 1319 | 13   | 91   | 18   | 0.00976  | 0.834862 |
| 33   | 1332 | 0    | 109  | 0    | 0        | 1        |

### 逻辑回归

用来解决二分类问题，选取一条曲线作为决策边界。

结果：

| 迭代次数 | FF   | FT   | TF   | TT   | 误登率   | 误杀率   |
| -------- | ---- | ---- | ---- | ---- | -------- | -------- |
| 10       | 1092 | 240  | 83   | 26   | 0.18018  | 0.761468 |
| 100      | 1088 | 244  | 82   | 27   | 0.183183 | 0.752294 |
| 1000     | 1088 | 244  | 82   | 27   | 0.183183 | 0.752294 |

### SVM

SVM通常用来解决多个维度上的二分类问题，相比于逻辑回归，更容易找到决策边界的超平面。通常可以选取的核函数有：rbf，sigmoid，linear

结果：

| 核函数  | gamma | 学习速率 | FF   | FT   | TF   | TT   | 误登率 | 误杀率 |
| ------- | ----- | -------- | ---- | ---- | ---- | ---- | ------ | ------ |
| RBF     | 20    | 0.8      | 1332 | 0    | 109  | 0    | 0      | 1      |
| sigmoid |       |          | 1332 | 0    | 109  | 0    | 0      | 1      |
| linear  |       | 0.1      | 1332 | 0    | 109  | 0    | 0      | 1      |

##  实验过程描述

1. 使用SVM等传统机器学习算法效果太差

   使用深度神经网络模型，效果更好。

2. 分类效果不理想，怀疑数据采集不正确

   使用python绘制散点图，结果显示，散点图的轮廓和在屏幕上写的字的形状大致相等。当快速写的时候，散点图稀疏，反之密集。得出结论，数据采集正确。

3. 使用网络模型训练速度太慢

   调整参数，在不影响实验结果的情况下将参数调到最简，简化模型，缩短训练时间。同时，将训练过程移至服务端，用更好的CPU/GPU运行。

4. 训练集正负类样本比例严重不均，正类样本只能是10个，负类样本远远高于正类样本。

   使用降采样，减少负类样本个数。调整得出最合适的正负类样本比例。也可以将已经成功登录的数据当成训练集，增加正类样本个数（此功能尚未实现）。

5. LSTM做完归一化之后，效果更差

   猜想：将XY坐标放缩至[0,1]之间，降低了数据的分散程度，使得分类变得更加困难。

6. 在训练过程中，只迭代很少的次数，损失函数就收敛了。

   猜想：不同类的数据间隔太大，很容易分类，只需要很少的迭代就能分出结果。

7. 可以使用批处理来训练模型，可以提高训练速度